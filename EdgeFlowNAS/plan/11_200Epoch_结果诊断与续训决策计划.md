# 11 200 Epoch 结果诊断与续训决策计划
更新时间: 2026-02-19

## 0. 输入与边界
1. 输入日志窗口 A: `epoch=95~103`（旧裁剪阶段，`clip_rate=1.0000`）。  
2. 输入日志窗口 B: `epoch=164~200`（新裁剪阶段，batch 级一次 clip，`grad_clip_global_norm=200`）。  
3. 本文仅讨论超网训练过程，不讨论训练后子网搜索与最终子网选择。  

---

## 1. 结论速览
1. 旧问题“clip 全程饱和”已解除。  
2. `mean_epe_12` 有改善，但进入平台期。窗口 A（95~103）约在 `9.749`，窗口 B（164~200）约在 `9.744`，改善约 `0.005`。  
3. `std_epe_12` 明显收敛到更低区间（多数在 `0.005~0.010`），尾部抖动显著小于早期。  
4. `arch_rank_12` 排名变稳定：头部常见 `3/1/0/5/10`，尾部多为 `2/9`。  
5. 当前主要瓶颈已从“梯度被过度截断”转为“学习率处于余弦尾段，推进速度慢”。  
6. `clip_count` 旧阶段与新阶段不能直接横向对比，因为统计口径已从旧实现切到 batch 级触发统计。  

---

## 2. 详细诊断

### 2.1 训练动力学
1. `loss` 从约 `7.x` 下降到 `6.0~6.5` 区间，说明训练仍在优化。  
2. `grad_norm` 大多在 `100~140`，`grad_p90` 常见 `170~270`，偶发更高值（如 `353`）。  
3. `clip_rate` 常见 `0.04~0.22`，不再是 100% 触发，裁剪强度已处在可用区间。  

### 2.2 Eval 行为
1. `mean_epe_12` 在后段主要波动于 `9.7418~9.7470`。  
2. 最优值出现在 `epoch=192`（`9.741824`），`epoch=200` 为 `9.742316`，基本维持在最优附近。  
3. 结论：无明显退化，但提升幅度已很小。  

### 2.3 排名行为
1. 排名在后段明显稳定，头尾结构已基本固定。  
2. 以 `epoch=200` 为例，最优/最差仅差约 `0.0156`，说明 12 子网在该固定 eval 口径下差距已较小。  
3. 差距变小有两种解释并存：  
- 正向：权重共享训练更均衡。  
- 风险：固定小 eval 切片上“可分辨率”下降，真实泛化差异被压缩。  

### 2.4 当前尚存问题
1. 指标平台化：`mean_epe_12` 继续下降空间有限。  
2. 评估口径虽一致，但覆盖仍偏窄（12 子网 x 4 batch 固定切片）。  
3. 控制台日志未直接打印 `lr`，不利于解释“调度位置 -> 指标变化”。  
4. `early_stop_min_delta=0.002` 在当前后段偏粗，可能忽略 `1e-3` 量级改善。  

---

## 3. 是否需要再训练 100 Epoch
结论: 可以继续，但不建议无门槛直接 +100；建议“条件式续训”。  

支持继续的理由:
1. 你的判断成立：前期旧 clip 阶段有效更新受限，可视为“低效热身”。  
2. 200 epoch 末端已接近余弦尾部，继续训练若采用合理学习率策略，仍有机会挖出小幅增益。  

反对盲目继续的理由:
1. 目前改善已进入小数点后三位量级，收益不确定。  
2. 若直接改 `num_epochs=300` 且保持 `base_lr=1e-4`，余弦调度会出现“回温”（相对当前 200 轮设定），存在重新放大波动的风险。  

---

## 4. +100 续训选项（含优缺点）

### 选项 A: 直接续到 300，保持 `lr=1e-4` + cosine
优点:
1. 改动最小，操作最简单。  
2. 恢复后有效学习率会回升，更新力度更大。  

缺点:
1. 调度轨迹发生跳变（与原 200 epoch 方案不可直接同口径对比）。  
2. 可能重新引入尾部子网波动。  

### 选项 B: 续到 300，但下调 base lr（如 `5e-5` 或 `4e-5`）+ cosine
优点:
1. 仍有回温，但强度可控，稳定性更好。  
2. 更符合当前“已到后段、只需细调”的状态。  

缺点:
1. 收敛速度更慢。  
2. 若模型仍欠拟合，可能推进不足。  

### 选项 C: 分段续训（推荐）: `+30` 观测，再决定 `+70`
优点:
1. 先用小成本验证“是否还有可挖收益”。  
2. 可用门槛规则避免无效长跑。  

缺点:
1. 管理步骤更多。  
2. 需要明确停止/继续标准。  

### 选项 D: 停在 200，不再续训
优点:
1. 节省算力和时间。  
2. 当前结果已稳定可复现。  

缺点:
1. 可能放弃因前期过强 clip 而被延后的那部分可恢复收益。  

---

## 5. 推荐执行方案（B + C 组合）

### P0. 先跑 30 epoch 试验段（201~230）
1. 从 `epoch=200` checkpoint 恢复。  
2. 设 `num_epochs=230`。  
3. `base_lr` 先用 `5e-5`（若更保守可用 `4e-5`）。  
4. 保持其余条件不变：`batch_size/micro_batch_size/clip=200/eval_batches_per_arch=4/bn_recal_batches`。  

### P1. 继续门槛（是否再跑到 300）
满足以下 3 条中的 2 条，再继续 +70:
1. 试验段内 `best mean_epe_12` 相比 `epoch=200` 至少再降 `>=0.0010`。  
2. `std_epe_12` 中位数 `<=0.008` 且无连续异常放大。  
3. 尾部不恶化（`arch_rank_12` 的尾部成员不出现持续性漂移和爆点）。  

若不满足:
1. 停止续训。  
2. 将 `epoch=192` 与 `epoch=200` 的 checkpoint 作为候选基线，进入后续阶段（不在本文展开）。  

---

## 6. 建议补充的低成本日志（可选）
1. 在 epoch 日志里增加 `lr=...`（强烈建议）。  
2. 增加 `best_so_far_mean_epe` 与 `delta_to_best`。  
3. 增加 `tail_gap = worst_arch_epe - median_arch_epe`。  

说明:
这些日志均为轻量统计，不会明显影响训练效率。  

---

## 7. 兼容性与恢复说明
1. 当前方案不改模型参数形状，checkpoint 兼容。  
2. 可继续无缝断点恢复。  
3. 若修改 `num_epochs`（例如 200 -> 230/300），需要在实验记录中标注“调度总步数变化点”，避免误读曲线。  

---

## 8. 12 子网结构、选取依据与当前排名解读（补充）

### 8.1 `arch_code` 的结构含义
1. 9 维编码前 4 位是骨干深度选择：`EB0/EB1/DB0/DB1`，`0/1/2 = Deep1/Deep2/Deep3`。  
2. 后 5 位是 head 卷积核选择：`H0Out/H1/H1Out/H2/H2Out`，`0/1/2 = 7x7/5x5/3x3`。  
3. 深度分支在实现上是包含关系：`Deep2` 在 `Deep1` 基础上再叠一层，`Deep3` 在 `Deep2` 上再叠一层。  
4. 因此，`arch_idx` 对应的是“固定结构模板”，`arch_rank_12` 里的数字是该模板在 eval_pool 中的索引，不是随机编号。  

实现与定义来源:
- `MCUFlowNet/EdgeFlowNAS/code/nas/arch_codec.py`
- `MCUFlowNet/EdgeFlowNAS/code/network/MultiScaleResNet_supernet.py`
- `MCUFlowNet/EdgeFlowNAS/code/nas/eval_pool_builder.py`

### 8.2 当前 12 个子网分别是什么结构
基于 `eval_pool_12.json`（`seed=42`, `size=12`）：

1. `arch_idx=0`  
`arch_code=[0,0,0,0,0,0,0,0,0]`  
骨干: `EB0=Deep1, EB1=Deep1, DB0=Deep1, DB1=Deep1`  
Head: `H0Out=7x7, H1=7x7, H1Out=7x7, H2=7x7, H2Out=7x7`

2. `arch_idx=1`  
`arch_code=[1,1,1,1,1,1,1,1,1]`  
骨干: 全 `Deep2`  
Head: 全 `5x5`

3. `arch_idx=2`  
`arch_code=[2,2,2,2,2,2,2,2,2]`  
骨干: 全 `Deep3`  
Head: 全 `3x3`

4. `arch_idx=3`  
`arch_code=[0,1,2,0,1,2,0,1,2]`  
骨干: `Deep1/Deep2/Deep3/Deep1`  
Head: `5x5/3x3/7x7/5x5/3x3`

5. `arch_idx=4`  
`arch_code=[2,1,0,2,1,0,2,1,0]`  
骨干: `Deep3/Deep2/Deep1/Deep3`  
Head: `5x5/7x7/3x3/5x5/7x7`

6. `arch_idx=5`  
`arch_code=[1,2,0,1,2,0,1,2,0]`  
骨干: `Deep2/Deep3/Deep1/Deep2`  
Head: `3x3/7x7/5x5/3x3/7x7`

7. `arch_idx=6`  
`arch_code=[2,0,0,2,1,0,0,0,2]`  
骨干: `Deep3/Deep1/Deep1/Deep3`  
Head: `5x5/7x7/7x7/7x7/3x3`

8. `arch_idx=7`  
`arch_code=[0,2,2,2,0,2,1,0,0]`  
骨干: `Deep1/Deep3/Deep3/Deep3`  
Head: `7x7/3x3/5x5/7x7/7x7`

9. `arch_idx=8`  
`arch_code=[0,0,0,2,2,0,2,0,2]`  
骨干: `Deep1/Deep1/Deep1/Deep3`  
Head: `3x3/7x7/3x3/7x7/3x3`

10. `arch_idx=9`  
`arch_code=[2,2,2,1,0,1,2,1,0]`  
骨干: `Deep3/Deep3/Deep3/Deep2`  
Head: `7x7/5x5/3x3/5x5/7x7`

11. `arch_idx=10`  
`arch_code=[0,2,1,1,1,0,0,1,0]`  
骨干: `Deep1/Deep3/Deep2/Deep2`  
Head: `5x5/7x7/7x7/5x5/7x7`

12. `arch_idx=11`  
`arch_code=[0,1,0,1,1,2,1,0,2]`  
骨干: `Deep1/Deep2/Deep1/Deep2`  
Head: `5x5/3x3/5x5/7x7/3x3`

### 8.3 为什么这样选这 12 个
1. 前 6 个是固定“种子结构”：
- 全 0、全 1、全 2（覆盖纯浅/纯中/纯深与纯大核/中核/小核极端）。
- 三种轮转模式（顺序轮转、逆序轮转、偏移轮转），用于覆盖“混合结构”。
2. 后 6 个是基于同一随机种子补全的互异随机结构，用于增加多样性。  
3. 选择目标是“低成本稳定监控”而不是“完整搜索代表性”：
- 保证每个 block 的 3 个选项在池中都出现至少一次（coverage ok）。
- 固定池内容，便于跨 epoch 比较趋势。  

### 8.4 从当前排名能看出什么（结合 201-222 日志）
1. 头部非常稳定：`arch_idx=3` 在 22/22 个 epoch 中均为第 1。  
2. 尾部非常稳定：末位主要是 `arch_idx=2`（19/22），其次 `arch_idx=9`（3/22）。  
3. 按平均名次看（201-222）：
- 头部: `3 > 1 > 5 > 0 > 10`
- 中部: `4 > 6 > 8 > 11 > 7`
- 尾部: `9 > 2`
4. 解释：
- 当前超网权重下，`arch_idx=3` 这类“深度与核大小混合、不过分极端”的结构最稳。  
- `arch_idx=2`（全 Deep3 + 全 3x3）长期垫底，说明该极端配置在当前训练动力学下不占优。  
- 排名稳定性提升是好事，但也提示评估已进入“细粒度差距区间”，后续应更多关注小幅改进是否可重复。  
