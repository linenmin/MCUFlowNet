#!/usr/bin/env python
"""
MultiScaleResNet_bilinear æ¨¡å‹å¯¼å‡ºè„šæœ¬
ä½¿ç”¨éšæœºæƒé‡ï¼Œç”¨äºæµ‹è¯• Axelera ç¼–è¯‘å™¨å…¼å®¹æ€§
"""

import os
import sys

# æ·»åŠ å¿…è¦çš„è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), 'sramTest'))

import tensorflow as tf
import numpy as np

# è®¾ç½® TensorFlow 1.x å…¼å®¹æ¨¡å¼
tf.compat.v1.disable_eager_execution()

# é…ç½®å‚æ•°
INPUT_HEIGHT = 576
INPUT_WIDTH = 1024
INPUT_CHANNELS = 6  # ä¸¤å¸§ RGB
OUTPUT_CHANNELS = 2
OUTPUT_ONNX_PATH = "multiscale_bilinear_576_1024.onnx"
EXPORT_NCHW = True
FORCE_EXPLICIT_PADS = True


def AccumPreds(prVals):
    """ç´¯ç§¯å¤šå°ºåº¦é¢„æµ‹è¾“å‡º (å¤ç”¨è‡ª misc/utils.py)"""
    prValAccum = None
    prValsAccum = []
    for prVali in prVals:
        if prValAccum == None:
            prValAccum = prVali
            prValsAccum.append(prValAccum)
            continue
                
        prValAccum = tf.compat.v1.image.resize_bilinear(
            prValAccum,
            [prVali.shape[1], prVali.shape[2]],
            align_corners=False,
            half_pixel_centers=True,
        )
        prValAccum += prVali
        prValsAccum.append(prValAccum)
    
    return prValAccum, prValsAccum


def create_model_with_random_weights():
    """åˆ›å»ºå¸¦éšæœºæƒé‡çš„æ¨¡å‹"""
    print("=" * 50)
    print("MultiScaleResNet_bilinear TensorFlow â†’ ONNX è½¬æ¢")
    print("=" * 50)
    
    # å¯¼å…¥æ¨¡å‹
    from sramTest.network.MultiScaleResNet_bilinear import MultiScaleResNet
    
    print(f"\n[1/4] åˆ›å»ºæ¨¡å‹ (è¾“å…¥: {INPUT_HEIGHT}x{INPUT_WIDTH}x{INPUT_CHANNELS})...")
    
    # åˆ›å»º TensorFlow ä¼šè¯å’Œå›¾
    tf.compat.v1.reset_default_graph()
    
    # è¾“å…¥å ä½ç¬¦ - NHWC æ ¼å¼
    input_ph = tf.compat.v1.placeholder(
        tf.float32, 
        shape=[1, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS],
        name='input'
    )
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MultiScaleResNet(
        InputPH=input_ph,
        Padding='same',
        NumOut=2,  # å…‰æµè¾“å‡º (u, v)
        InitNeurons=16,  # åˆå§‹é€šé“æ•°
        ExpansionFactor=2.0,
        NumSubBlocks=2,
        NumBlocks=1,
        Suffix='',
        UncType=None
    )
    
    # æ„å»ºç½‘ç»œ - è¿”å›å¤šå°ºåº¦è¾“å‡ºåˆ—è¡¨
    multi_scale_outputs = model.Network()
    print(f"      å¤šå°ºåº¦è¾“å‡ºæ•°é‡: {len(multi_scale_outputs)}")
    for i, out in enumerate(multi_scale_outputs):
        print(f"        å°ºåº¦ {i}: {out.shape}")
    
    # ä½¿ç”¨ AccumPreds ç´¯ç§¯å¤šå°ºåº¦è¾“å‡º
    accum_output, accum_outputs = AccumPreds(multi_scale_outputs)
    
    # å–å‰2ä¸ªé€šé“ (u, v å…‰æµ)
    main_output = accum_output[..., 0:OUTPUT_CHANNELS]
    main_output = tf.identity(main_output, name='output')
    
    print(f"      âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"      ç´¯ç§¯è¾“å‡ºå½¢çŠ¶: {main_output.shape}")
    
    return input_ph, main_output


def convert_to_onnx(input_tensor, output_tensor):
    """è½¬æ¢ä¸º ONNX æ ¼å¼"""
    import tf2onnx
    
    print(f"\n[2/4] åˆå§‹åŒ–éšæœºæƒé‡...")
    
    with tf.compat.v1.Session() as sess:
        # ä½¿ç”¨éšæœºæƒé‡åˆå§‹åŒ–
        sess.run(tf.compat.v1.global_variables_initializer())
        print("      âœ“ éšæœºæƒé‡åˆå§‹åŒ–å®Œæˆ")
        
        # æµ‹è¯•æ¨ç†
        print("\n[3/4] æµ‹è¯• TensorFlow æ¨ç†...")
        test_input = np.random.randn(1, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS).astype(np.float32)
        output_val = sess.run(output_tensor, feed_dict={input_tensor: test_input})
        print(f"      âœ“ æ¨ç†æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output_val.shape}")
        print(f"      è¾“å‡ºèŒƒå›´: [{output_val.min():.4f}, {output_val.max():.4f}]")
        
        # å†»ç»“å›¾ - åœ¨å½“å‰ Session ä¸­è¿›è¡Œ
        print(f"\n[4/4] å†»ç»“å›¾å¹¶è½¬æ¢ä¸º ONNX...")
        
        # è·å–è¾“å‡ºèŠ‚ç‚¹åç§° (å»æ‰ :0)
        output_node_name = output_tensor.name.split(':')[0]
        
        # å†»ç»“å›¾ - æŠŠå˜é‡è½¬æ¢ä¸ºå¸¸é‡
        frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(
            sess,
            sess.graph_def,
            [output_node_name]
        )
        print(f"      âœ“ å›¾å†»ç»“å®Œæˆ")
        
    # åœ¨ Session å¤–ç”¨å†»ç»“çš„å›¾è¿›è¡Œè½¬æ¢
    tf.compat.v1.reset_default_graph()
    
    onnx_model, _ = tf2onnx.convert.from_graph_def(
        frozen_graph_def,
        input_names=[input_tensor.name],
        output_names=[output_tensor.name],
        opset=17
    )
    if EXPORT_NCHW:
        onnx_model = strip_io_transposes(onnx_model)
    if FORCE_EXPLICIT_PADS:
        onnx_model = convert_auto_pad_to_explicit_pads(onnx_model)
    
    # ä¿å­˜
    import onnx
    onnx.save(onnx_model, OUTPUT_ONNX_PATH)
    print(f"      âœ“ ä¿å­˜åˆ°: {OUTPUT_ONNX_PATH}")
    
    return True


def _get_attr_ints(node, name):
    for attr in node.attribute:
        if attr.name == name:
            return list(attr.ints)
    return None


def _get_dims(vi):
    return [d.dim_value if d.dim_value else d.dim_param or None for d in vi.type.tensor_type.shape.dim]


def _set_dims(vi, dims):
    shape = vi.type.tensor_type.shape
    del shape.dim[:]
    for val in dims:
        dim = shape.dim.add()
        if isinstance(val, int):
            dim.dim_value = val
        elif val is not None:
            dim.dim_param = str(val)


def _reorder_nhwc_to_nchw(dims):
    if len(dims) != 4:
        return dims
    return [dims[0], dims[3], dims[1], dims[2]]


def strip_io_transposes(model):
    graph = model.graph
    input_name = graph.input[0].name
    output_name = graph.output[0].name

    output_to_node = {}
    for node in graph.node:
        for out_name in node.output:
            output_to_node[out_name] = node

    def _resolve_identity_chain(name):
        identities = []
        node = output_to_node.get(name)
        while node is not None and node.op_type == "Identity":
            identities.append(node)
            if not node.input:
                break
            name = node.input[0]
            node = output_to_node.get(name)
        return name, node, identities

    input_transpose = None
    for node in graph.node:
        if node.op_type != "Transpose":
            continue
        perm = _get_attr_ints(node, "perm")
        if perm == [0, 3, 1, 2] and node.input and node.input[0] == input_name:
            input_transpose = node
            break

    if input_transpose:
        trans_out = input_transpose.output[0]
        for node in graph.node:
            for idx, name in enumerate(node.input):
                if name == trans_out:
                    node.input[idx] = input_name
        graph.node.remove(input_transpose)

        in_dims = _get_dims(graph.input[0])
        _set_dims(graph.input[0], _reorder_nhwc_to_nchw(in_dims))

    orig_out_dims = _get_dims(graph.output[0])
    resolved_name, resolved_node, identity_nodes = _resolve_identity_chain(output_name)

    output_transpose = None
    if resolved_node is not None and resolved_node.op_type == "Transpose":
        perm = _get_attr_ints(resolved_node, "perm")
        if perm == [0, 2, 3, 1]:
            output_transpose = resolved_node

    if output_transpose:
        new_output = output_transpose.input[0]
        trans_out = output_transpose.output[0]

        for node in graph.node:
            for idx, name in enumerate(node.input):
                if name == trans_out:
                    node.input[idx] = new_output

        graph.output[0].name = new_output

        out_dims = None
        for vi in list(graph.value_info) + list(graph.output) + list(graph.input):
            if vi.name == new_output:
                out_dims = _get_dims(vi)
                break
        if out_dims:
            _set_dims(graph.output[0], out_dims)
        else:
            _set_dims(graph.output[0], _reorder_nhwc_to_nchw(orig_out_dims))

        graph.node.remove(output_transpose)

        for node in identity_nodes:
            if node in graph.node:
                graph.node.remove(node)

    if EXPORT_NCHW and len(graph.output) > 0:
        out_dims = _get_dims(graph.output[0])
        if (
            not out_dims
            or any(dim is None for dim in out_dims)
            or (len(out_dims) == 4 and out_dims[1] == INPUT_HEIGHT and out_dims[2] == INPUT_WIDTH and out_dims[3] == OUTPUT_CHANNELS)
        ):
            _set_dims(
                graph.output[0],
                [1, OUTPUT_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH],
            )

    return model


def _get_initializer_shape(model, name):
    for init in model.graph.initializer:
        if init.name == name:
            return list(init.dims)
    return None


def _get_shape_from_value_info(model, name):
    for vi in list(model.graph.value_info) + list(model.graph.input) + list(model.graph.output):
        if vi.name == name:
            return _get_dims(vi)
    return None


def convert_auto_pad_to_explicit_pads(model):
    import math
    import onnx
    from onnx import shape_inference

    model = shape_inference.infer_shapes(model)

    for node in model.graph.node:
        if node.op_type != "Conv":
            continue

        auto_pad = None
        pads_attr = None
        for attr in node.attribute:
            if attr.name == "auto_pad" and attr.s:
                auto_pad = attr.s.decode("utf-8")
            elif attr.name == "pads":
                pads_attr = list(attr.ints)

        if auto_pad is None or auto_pad == "NOTSET":
            continue

        in_shape = _get_shape_from_value_info(model, node.input[0])
        if not in_shape or len(in_shape) < 4 or any(dim is None for dim in in_shape[0:4]):
            continue

        kernel_shape = None
        for attr in node.attribute:
            if attr.name == "kernel_shape":
                kernel_shape = list(attr.ints)
                break
        if kernel_shape is None:
            weight_shape = _get_initializer_shape(model, node.input[1])
            if weight_shape and len(weight_shape) >= 4:
                kernel_shape = weight_shape[-2:]

        if not kernel_shape or len(kernel_shape) != 2:
            continue

        strides = None
        dilations = None
        for attr in node.attribute:
            if attr.name == "strides":
                strides = list(attr.ints)
            elif attr.name == "dilations":
                dilations = list(attr.ints)
        if not strides:
            strides = [1, 1]
        if not dilations:
            dilations = [1, 1]

        in_h, in_w = in_shape[2], in_shape[3]
        k_h, k_w = kernel_shape
        s_h, s_w = strides
        d_h, d_w = dilations

        if auto_pad == "VALID":
            pads = [0, 0, 0, 0]
        else:
            out_h = int(math.ceil(float(in_h) / float(s_h)))
            out_w = int(math.ceil(float(in_w) / float(s_w)))
            pad_h_total = max((out_h - 1) * s_h + (k_h - 1) * d_h + 1 - in_h, 0)
            pad_w_total = max((out_w - 1) * s_w + (k_w - 1) * d_w + 1 - in_w, 0)

            if auto_pad == "SAME_LOWER":
                pad_top = int(math.ceil(pad_h_total / 2.0))
                pad_left = int(math.ceil(pad_w_total / 2.0))
            else:
                pad_top = int(math.floor(pad_h_total / 2.0))
                pad_left = int(math.floor(pad_w_total / 2.0))

            pad_bottom = pad_h_total - pad_top
            pad_right = pad_w_total - pad_left

            pads = [pad_top, pad_left, pad_bottom, pad_right]

        new_attrs = []
        for attr in node.attribute:
            if attr.name in ("auto_pad", "pads"):
                continue
            new_attrs.append(attr)
        new_attrs.append(onnx.helper.make_attribute("pads", pads))
        new_attrs.append(onnx.helper.make_attribute("auto_pad", "NOTSET"))
        node.attribute[:] = new_attrs

    return model


def verify_onnx():
    """éªŒè¯ ONNX æ¨¡å‹"""
    import onnx
    import onnxruntime as ort
    
    print("\n[éªŒè¯] ONNX æ¨¡å‹æ£€æŸ¥...")
    
    # åŠ è½½å¹¶æ£€æŸ¥
    model = onnx.load(OUTPUT_ONNX_PATH)
    onnx.checker.check_model(model)
    print("      âœ“ ONNX æ¨¡å‹ç»“æ„æœ‰æ•ˆ")
    
    # æ¨ç†æµ‹è¯•
    print("      æµ‹è¯• ONNX Runtime æ¨ç†...")
    sess = ort.InferenceSession(OUTPUT_ONNX_PATH)
    
    # è·å–è¾“å…¥è¾“å‡ºåç§°
    input_name = sess.get_inputs()[0].name
    output_name = sess.get_outputs()[0].name
    
    if EXPORT_NCHW:
        test_input = np.random.randn(1, INPUT_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH).astype(np.float32)
    else:
        test_input = np.random.randn(1, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS).astype(np.float32)
    result = sess.run([output_name], {input_name: test_input})
    
    print(f"      âœ“ æ¨ç†æˆåŠŸ")
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {result[0].shape}")
    print(f"  è¾“å‡ºèŒƒå›´: [{result[0].min():.4f}, {result[0].max():.4f}]")
    print("\nğŸ‰ æ¨¡å‹æ¨ç†æµ‹è¯•é€šè¿‡!")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ¨¡å‹
    input_tensor, output_tensor = create_model_with_random_weights()
    
    # è½¬æ¢ä¸º ONNX
    convert_to_onnx(input_tensor, output_tensor)
    
    # éªŒè¯
    verify_onnx()
    
    print("\n" + "=" * 50)
    print("å®Œæˆï¼è¯·ä¸Šä¼ åˆ° OrangePi æµ‹è¯•ç¼–è¯‘ï¼š")
    print(f"  scp {OUTPUT_ONNX_PATH} orangepi@orangepi5plus:~/.cache/axelera/weights/")
    print("=" * 50)


if __name__ == "__main__":
    main()
