{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd3e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 试验：按需现算 proxy 训练/验证\n",
    "# 可配置：任务/搜索空间、特征列表（proxy + 可选架构哈希）、\n",
    "# 训练采样（random），验证采样（random/percent/flops），batch_size/maxbatch/decoder_only 等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f258e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random  # 基础库\n",
    "from pathlib import Path  # 路径处理\n",
    "import numpy as np  # 数值\n",
    "import pandas as pd  # 表格\n",
    "import torch  # 深度学习\n",
    "from tqdm import tqdm  # 进度条\n",
    "\n",
    "# xgboost 依赖\n",
    "import xgboost as xgb  # 回归/排序模型\n",
    "\n",
    "# 项目路径与导入（notebook 下无 __file__，用 cwd 回退）\n",
    "try:\n",
    "    CURRENT_DIR = Path(__file__).resolve().parent  # 脚本目录\n",
    "except NameError:\n",
    "    CURRENT_DIR = Path.cwd()  # notebook 回退为当前目录\n",
    "ROOT_DIR = CURRENT_DIR.parent  # 项目根\n",
    "NASLIB_ROOT = ROOT_DIR / \"NASLib\"  # NASLib 根\n",
    "sys.path.insert(0, str(ROOT_DIR))  # 加入路径\n",
    "sys.path.insert(0, str(NASLIB_ROOT))  # 加入 NASLib\n",
    "\n",
    "from proxy_TransNAS.utils.load_model import (\n",
    "    load_transbench_classes,  # 搜索空间类\n",
    "    load_transbench_api,  # API 缓存\n",
    "    make_train_loader,  # loader 构造\n",
    "    truncate_loader,  # 截断 batch\n",
    "    select_architectures_by_percentile,  # 百分位采样\n",
    "    get_metric_name,  # metric 名\n",
    "    set_op_indices_from_str,  # 写入 op_indices\n",
    ")\n",
    "from proxy_TransNAS.proxies.factory import compute_proxy_score  # 统一 proxy 调用\n",
    "from proxy_TransNAS.proxies.zico import get_loss_fn  # 取 loss\n",
    "from naslib import utils as nas_utils  # NASLib 工具\n",
    "\n",
    "get_train_val_loaders = nas_utils.get_train_val_loaders  # 备用接口\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3e6a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x177df3b1bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置区：按需修改\n",
    "CFG = {\n",
    "    # 训练集采样（目前仅 random）\n",
    "    \"train\": {\n",
    "        \"sample_mode\": \"random\",  # 仅支持 random\n",
    "        \"num_samples\": 100,  # 训练采样数量\n",
    "    },\n",
    "    # 验证集采样方式：random / percent / flops\n",
    "    \"val\": {\n",
    "        \"sample_mode\": \"random\",  # 可改为 percent / flops\n",
    "        \"num_samples\": 500,         # random 时有效\n",
    "        \"start_percent\": 0.0,      # percent 时起点\n",
    "        \"end_percent\": 10.0,       # percent 时终点\n",
    "        \"flops_csv\": str(ROOT_DIR / \"proxy_TransNAS\" / \"flops_lookup\" / \"flops_macro_autoencoder.csv\"),  # flops 模式 CSV\n",
    "        \"start_arch_str\": True,    # flops 模式起点架构\n",
    "        \"arch_count\": 20,          # flops 模式数量\n",
    "    },\n",
    "    # 任务与搜索空间\n",
    "    \"tasks\": [\"autoencoder\"],           # 可选: autoencoder / segmentsemantic / normal\n",
    "    \"search_space\": \"macro\",          # macro / micro\n",
    "    # 特征列表：可包含 proxies 中的任意名字，以及可选 \"arch_hash\"\n",
    "    # \"features\": [\"flops\", \"naswot\", \"swap\", \"zico\", \"fisher\"],\n",
    "    \"features\": [\"flops\", \"fisher\"],  # 当前使用的特征\n",
    "    \"use_arch_hash\": False,             # 是否把架构码转数值特征\n",
    "    # 计算相关配置\n",
    "    \"decoder_only\": False,  # 仅 decoder\n",
    "    \"batch_size\": 16,       # DataLoader batch\n",
    "    \"maxbatch\": 2,          # 截断 batch 数\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",  # 设备\n",
    "    \"seed\": 42,             # 随机种子\n",
    "}\n",
    "\n",
    "random.seed(CFG[\"seed\"])\n",
    "np.random.seed(CFG[\"seed\"])\n",
    "torch.manual_seed(CFG[\"seed\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea2c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实用函数\n",
    "TransBench101SearchSpaceMicro, TransBench101SearchSpaceMacro, graph_module = load_transbench_classes()  # 搜索空间类\n",
    "Metric = graph_module.Metric  # metric 类\n",
    "\n",
    "def arch_to_num(arch_str: str) -> float:\n",
    "    \"\"\"把架构串哈希为稳定的数值特征。\"\"\"\n",
    "    return float(abs(hash(arch_str)) % (10**12)) / 1e12  # 归一化哈希\n",
    "\n",
    "def sample_arch_strings(dataset_api, task: str, search_space: str, mode: str, cfg_val):\n",
    "    api = dataset_api[\"api\"]  # API 对象\n",
    "    if mode == \"random\":\n",
    "        pool = api.all_arch_dict[search_space]  # 架构池\n",
    "        k = min(cfg_val[\"num_samples\"], len(pool))  # 取样数量\n",
    "        return random.sample(pool, k)  # 随机采样\n",
    "    elif mode == \"percent\":\n",
    "        return select_architectures_by_percentile(\n",
    "            dataset_api, search_space, task, cfg_val[\"start_percent\"], cfg_val[\"end_percent\"],\n",
    "        )  # 百分位采样\n",
    "    elif mode == \"flops\":\n",
    "        df = pd.read_csv(cfg_val[\"flops_csv\"]).sort_values(by=\"flops\").reset_index(drop=True)  # 读 flops CSV\n",
    "        if cfg_val[\"start_arch_str\"] is None:\n",
    "            start_idx = 0  # 默认起点\n",
    "        else:\n",
    "            hit = df.index[df[\"arch_str\"] == cfg_val[\"start_arch_str\"]].tolist()  # 找起点\n",
    "            assert len(hit) > 0, \"start_arch_str 未找到\"\n",
    "            start_idx = hit[0]\n",
    "        end_idx = start_idx + cfg_val[\"arch_count\"]  # 终点\n",
    "        return df.iloc[start_idx:end_idx][\"arch_str\"].tolist()  # 切片\n",
    "    else:\n",
    "        raise ValueError(f\"未知采样模式: {mode}\")\n",
    "\n",
    "\n",
    "def build_graph(ss_name, task, arch_str):\n",
    "    if ss_name == \"micro\":\n",
    "        if task == \"segmentsemantic\":\n",
    "            ss = TransBench101SearchSpaceMicro(dataset=task, create_graph=True, n_classes=17)  # seg 特殊类别数\n",
    "        else:\n",
    "            ss = TransBench101SearchSpaceMicro(dataset=task, create_graph=True)\n",
    "    else:\n",
    "        ss = TransBench101SearchSpaceMacro(dataset=task, create_graph=True)\n",
    "    graph = ss.clone()  # 克隆\n",
    "    graph = set_op_indices_from_str(ss_name, graph, arch_str)  # 写入架构\n",
    "    graph.parse()  # 解析为 model\n",
    "    return graph\n",
    "\n",
    "\n",
    "def prepare_data_and_loss(task, device, batch_size, maxbatch):\n",
    "    train_loader = make_train_loader(task, Path(CFG[\"train\"].get(\"data_root\", NASLIB_ROOT / \"data\")), batch_size, CFG[\"seed\"])  # loader\n",
    "    train_batches = truncate_loader(train_loader, maxbatch)  # 截断 batch\n",
    "    loss_fn = get_loss_fn(task).to(device)  # 任务损失\n",
    "    return train_batches, loss_fn\n",
    "\n",
    "\n",
    "def compute_features_for_arch(model, arch_str, features, train_batches, loss_fn, device, decoder_only):\n",
    "    feats = {}  # 存储特征\n",
    "    for proxy_name in features:\n",
    "        score = compute_proxy_score(model, proxy_name, train_batches, loss_fn, device, decoder_only=decoder_only)  # 计算 proxy\n",
    "        feats[proxy_name] = float(score) if score is not None else 0.0  # 落地\n",
    "    if CFG[\"use_arch_hash\"]:\n",
    "        feats[\"arch_hash\"] = arch_to_num(arch_str)  # 加入哈希\n",
    "    return feats\n",
    "\n",
    "\n",
    "def collect_split(split_name: str, cfg_split, dataset_api, task, search_space, features, device, decoder_only, batch_size, maxbatch):\n",
    "    arch_strings = sample_arch_strings(dataset_api, task, search_space, cfg_split[\"sample_mode\"], cfg_split)  # 采样\n",
    "    if len(arch_strings) == 0:\n",
    "        print(f\"[{split_name}] {task} 无样本，跳过\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    train_batches, loss_fn = prepare_data_and_loss(task, device, batch_size, maxbatch)  # 数据与损失\n",
    "\n",
    "    rows = []\n",
    "    for arch_str in tqdm(arch_strings, desc=f\"[{split_name}-{task}]\", unit=\"arch\", disable=False, leave=False):\n",
    "        try:\n",
    "            graph = build_graph(search_space, task, arch_str)  # 构图\n",
    "            model = graph.to(device)  # 上设备\n",
    "            feats = compute_features_for_arch(model, arch_str, features, train_batches, loss_fn, device, decoder_only)  # 计算特征\n",
    "            metric_name = get_metric_name(task)  # metric 名\n",
    "            api = dataset_api[\"api\"]\n",
    "            gt = api.get_single_metric(arch_str, task, metric_name, mode=\"final\")  # 取 GT\n",
    "            feats.update({\"arch_str\": arch_str, \"task\": task, \"gt\": float(gt)})  # 补字段\n",
    "            rows.append(feats)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"OOM 跳过 {arch_str}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "        finally:\n",
    "            if 'model' in locals():\n",
    "                model.cpu()\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b95fb515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (100, 5)\n",
      "val shape: (500, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(       flops    fisher         arch_str         task        gt\n",
       " 0  22.399253  0.000025  64-441114-basic  autoencoder  0.502551\n",
       " 1  23.204164  0.000024   64-14213-basic  autoencoder  0.569561\n",
       " 2  22.676382  0.000018    64-3421-basic  autoencoder  0.540097\n",
       " 3  21.865235  0.000022  64-414341-basic  autoencoder  0.423320\n",
       " 4  21.432799  0.000020  64-334111-basic  autoencoder  0.497628,\n",
       "        flops    fisher         arch_str         task        gt\n",
       " 0  21.558844  0.000025  64-331124-basic  autoencoder  0.472741\n",
       " 1  23.158980  0.000016  64-121331-basic  autoencoder  0.591058\n",
       " 2  21.516149  0.000015   64-31431-basic  autoencoder  0.502346\n",
       " 3  21.652650  0.000031  64-314321-basic  autoencoder  0.508427\n",
       " 4  24.372371  0.000021  64-111242-basic  autoencoder  0.618440)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建训练 / 验证数据\n",
    "all_train = []  # 训练集列表\n",
    "all_val = []    # 验证集列表\n",
    "\n",
    "data_root = Path(NASLIB_ROOT / \"data\")  # 数据根\n",
    "\n",
    "for task in CFG[\"tasks\"]:  # 遍历任务\n",
    "    dataset_api = load_transbench_api(data_root, task)  # 载入 API\n",
    "    # train（仅 random）\n",
    "    df_tr = collect_split(\n",
    "        split_name=\"train\",\n",
    "        cfg_split=CFG[\"train\"],\n",
    "        dataset_api=dataset_api,\n",
    "        task=task,\n",
    "        search_space=CFG[\"search_space\"],\n",
    "        features=CFG[\"features\"],\n",
    "        device=CFG[\"device\"],\n",
    "        decoder_only=CFG[\"decoder_only\"],\n",
    "        batch_size=CFG[\"batch_size\"],\n",
    "        maxbatch=CFG[\"maxbatch\"],\n",
    "    )\n",
    "    if len(df_tr):\n",
    "        all_train.append(df_tr)\n",
    "    # val（可 random/percent/flops）\n",
    "    df_val = collect_split(\n",
    "        split_name=\"val\",\n",
    "        cfg_split=CFG[\"val\"],\n",
    "        dataset_api=dataset_api,\n",
    "        task=task,\n",
    "        search_space=CFG[\"search_space\"],\n",
    "        features=CFG[\"features\"],\n",
    "        device=CFG[\"device\"],\n",
    "        decoder_only=CFG[\"decoder_only\"],\n",
    "        batch_size=CFG[\"batch_size\"],\n",
    "        maxbatch=CFG[\"maxbatch\"],\n",
    "    )\n",
    "    if len(df_val):\n",
    "        all_val.append(df_val)\n",
    "\n",
    "train_df = pd.concat(all_train, ignore_index=True) if all_train else pd.DataFrame()  # 合并训练\n",
    "val_df = pd.concat(all_val, ignore_index=True) if all_val else pd.DataFrame()        # 合并验证\n",
    "print(\"train shape:\", train_df.shape)\n",
    "print(\"val shape:\", val_df.shape)\n",
    "train_df.head(), val_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc1d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val kendall=0.3221, spearman=0.4638, mse=0.0052\n",
      "\n",
      "单 proxy 对比 (val 集)：\n",
      "  flops         kendall=0.5728, spearman=0.7763\n",
      "  fisher        kendall=-0.2821, spearman=-0.4162\n"
     ]
    }
   ],
   "source": [
    "# 训练并验证 XGBoost 回归\n",
    "from scipy.stats import kendalltau, spearmanr  # 相关性\n",
    "\n",
    "def eval_rank(y_true, y_pred):\n",
    "    kt = kendalltau(y_true, y_pred, nan_policy=\"omit\").correlation  # Kendall\n",
    "    sp = spearmanr(y_true, y_pred, nan_policy=\"omit\").correlation  # Spearman\n",
    "    return kt, sp\n",
    "\n",
    "if len(train_df) == 0 or len(val_df) == 0:\n",
    "    print(\"训练或验证数据为空，检查采样配置\")\n",
    "else:\n",
    "    feature_cols = [c for c in train_df.columns if c not in [\"gt\", \"arch_str\", \"task\"]]  # 特征列\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"gt\"]\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[\"gt\"]\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=500,       # 树数\n",
    "        learning_rate=0.05,    # 学习率\n",
    "        max_depth=8,           # 深度\n",
    "        subsample=0.9,         # 行采样\n",
    "        colsample_bytree=0.9,  # 列采样\n",
    "        objective=\"reg:squarederror\",  # 回归\n",
    "        random_state=CFG[\"seed\"],\n",
    "    )\n",
    "    model.fit(X_train, y_train)  # 训练\n",
    "\n",
    "    val_pred = model.predict(X_val)  # 预测\n",
    "    kt, sp = eval_rank(y_val, val_pred)\n",
    "    mse = np.mean((val_pred - y_val) ** 2)\n",
    "    print(f\"val kendall={kt:.4f}, spearman={sp:.4f}, mse={mse:.4f}\")\n",
    "\n",
    "    # 对比单 proxy 的排名相关性\n",
    "    print(\"\\n单 proxy 对比 (val 集)：\")\n",
    "    for col in feature_cols:\n",
    "        kt_single, sp_single = eval_rank(y_val, val_df[col])\n",
    "        print(f\"  {col:12s}  kendall={kt_single:.4f}, spearman={sp_single:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
